# Topic-FlipRAG:Topic-FlipRAG: Topic-Orientated Adversarial Opinion Manipulation Attacks to Retrieval-Augmented Generation Models


![Python Version](https://img.shields.io/badge/python-3.9%2B-blue)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## Overview

This repository contains the implementation of Topic-FlipedRAG, a novel two-stage adversarial attack framework targeting Retrieval-Augmented Generation (RAG) systems. The proposed method demonstrates how strategic knowledge poisoning can systematically manipulate LLM outputs for opinion-oriented tasks through semantic-level perturbations.

## Key Features
- ðŸŽ¯ **Topic-oriented attacks** on multi-perspective content generation
- âš¡ **Dual-phase manipulation** combining:
  - Traditional adversarial ranking techniques
  - LLM-driven semantic perturbation generation
- ðŸ“Š Comprehensive evaluation framework for opinion shift measurement

## Installation

```bash
git clone https://github.com/your_anonymous_repo/Topic-FlipedRAG.git
cd Topic-FlipedRAG
pip install -r requirements.txt
```

**Requirements**:
- Python 3.9+
- PyTorch 2.0+
- Transformers 4.30+
- FAISS 1.7.2+
- (Complete with your actual dependencies)

## Usage

### Basic Attack Pipeline
```python
from attack_pipeline import TopicFlipedRAG

# Initialize attack module
attack_config = {
    "target_topic": "climate_change",
    "opinion_direction": "skepticism",
    "perturbation_level": 0.3
}
attacker = TopicFlipedRAG(**attack_config)

# Execute attack on RAG system
compromised_responses = attacker.execute_attack(
    base_retriever=your_retriever,
    generator_model=your_llm,
    query_batch=test_queries
)
```

### Evaluation Metrics
```python
from evaluation import OpinionShiftAnalyzer

analyzer = OpinionShiftAnalyzer(reference_corpus="neutral_responses.json")
shift_scores = analyzer.calculate_opinion_shift(
    original_responses=baseline_outputs,
    attacked_responses=compromised_responses
)
```

## Experimental Results

Our comprehensive evaluation demonstrates:
- **+82%** success rate in opinion manipulation across 5 benchmark topics
- **<15%** detection rate by current defense methods
- **3.2x** amplification effect in multi-query scenarios

(Replace with your actual experimental metrics)

## Contributing

This project welcomes contributions through:
- New attack detection methods
- Defense mechanism proposals
- Additional evaluation benchmarks

Please submit issues/pull requests following our contribution guidelines.

## License

MIT License (see [LICENSE](LICENSE) for details)

---

**Disclaimer**: This implementation is provided for research purposes only. Users must adhere to ethical AI guidelines and applicable laws when using this code.
